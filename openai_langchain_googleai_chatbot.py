# -*- coding: utf-8 -*-
"""OpenAI-LangChain-GoogleAI-Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krU3FbNFCJ9c5UWJOMMzu1bl4KkWvLPZ

##Using OpenAI LLMs of GPT 3.5 turbo
"""

!pip install openai

import openai

openai.api_key = "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"

def chat_with_gpt(prompt):
  response = openai.ChatCompletion.create(
      model="gpt-3.5-turbo",
      messages=[{"role": "user", "content": prompt}]
  )
  return response.choices[0].message.content.strip()

  if _name_ == "_main_":
    while True:
      user_input = input("You : ")
      if user_input.lower() in ["quite" , "exit" , "bye"]:
        break

      response = chat_with_gpt(user_input)
      print("Chatbot:", response)

"""##HuggingFace Tranformer models: Using LangChain"""

!pip install langchain

from langchain import HuggingFaceHub
from langchain import PromptTemplate, LLMChain
repo_id = "tiiuae/falcon-7b-instruct"
huggingfacehub_api_token = "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"
llm = HuggingFaceHub(huggingfacehub_api_token=huggingfacehub_api_token,
                     repo_id=repo_id,
                     model_kwargs={"temparature": 0.7 , "max_new_tokens": 500})

from re import template
template = """Question:{question}
             Answer: Let's give a detailed answer."""
prompt = PromptTemplate(template=template, input_variables=["question"])

chain = LLMChain(prompt=prompt, llm=llm)

out = chain.run("Give me a good receipe to make a cup cake.")
print(out)

from re import template
template = """Question:{question}
             Answer: Let's give a detailed answer with logical deductions in a precise manner like a mathematician who has expertise in geometry."""
prompt = PromptTemplate(template=template, input_variables=["question"])
chain = LLMChain(prompt=prompt, llm=llm)
out = chain.run("Give me the number theoretical proof of Pythagoras Theorem.")
print(out)

!pip install huggingface_hub
!pip install transformers
!pip install langchain
!pip install chainLit

import os
import chainlit as cl
from langchain import HuggingFaceHub, PromptTemplate, LLMChain

from getpass import getpass
huggingfacehub_api_token = getpass()
os.environ['huggingfacehub_api_token'] = huggingfacehub_api_token

# Setting the Conversational Model

model_id = "gpt2-medium" #380M parameters
conv_model = HuggingFaceHub(huggingfacehub_api_token = os.environ['huggingfacehub_api_token'],
                           repo_id=model_id,
                           model_kwargs={"temperature": 0.8 , "max_new_tokens": 200})

template = """You are a helpul AI assistant that makes stories by completing the query provided by the user
              {query}
           """
prompt = PromptTemplate(template=template, input_variables=['query'])

conv_chain = LLMChain(llm=conv_model,
                      prompt=prompt,
                      verbose=True)
print(conv_chain.run("Once upon a time in India, there was a emperor Asoka who had great reign..."))

"""## Using Google AI : Gemini 1.5 pro the next version of BARD"""

!pip install -q -U google-generativeai

import pathlib
import textwrap

import google.generativeai as genai

from IPython.display import display
from IPython.display import Markdown


def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)

"""Now I am ready to call the Gemini API.
####Use list_models to see the available Gemini models:
* gemini-1.5-pro: optimized for high intelligence tasks, the most powerful Gemini model
* gemini-1.5-flash: optimized for multi-modal use-cases where speed and cost are important

####The genai package also supports the PaLM family of models, but only the Gemini models support the generic, multimodal capabilities of the generateContent method.
"""

for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

"""##Generate text from text inputs
####For text-only prompts, use the gemini-pro model:
"""

model = genai.GenerativeModel('gemini-1.5-flash')

"""The generate_content method can handle a wide variety of use cases, including multi-turn chat and multimodal input, depending on what the underlying model supports. The available models only support text and images as input, and text as output.

#### In the simplest case, one can pass a prompt string to the GenerativeModel.generate_content method:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response = model.generate_content("What is the meaning of life?")

"""In simple cases, the response.text accessor is all I need. To display formatted Markdown text, use the to_markdown function:"""

to_markdown(response.text)

"""If the API failed to return a result, use `GenerateContentResponse.prompt_feedback` to see if it was blocked due to safety concerns regarding the prompt."""

response.prompt_feedback

"""####Gemini can generate multiple possible responses for a single prompt.
These possible responses are called candidates, and one can review them to select the most suitable one as the response.
View the response candidates with `GenerateContentResponse.candidates`:
"""

response.candidates

"""####By default, the model returns a response after completing the entire generation process.
One can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.
To stream responses, use `GenerativeModel.generate_content(..., stream=True)`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response = model.generate_content("What is the meaning of life?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

response = model.generate_content("What is the meaning of life?", stream=True)

response.prompt_feedback

try:
  response.text
except Exception as e:
  print(f'{type(e).__name__}: {e}')

"""##Generate text from image and text inputs

The `GenerativeModel.generate_content` API is designed to handle multimodal prompts and returns a text output.
Let include an image:
"""

!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw

import PIL.Image

img = PIL.Image.open('image.jpg')
img

"""Use the `gemini-1.5-flash` model and pass the image to the model with `generate_content`."""

model = genai.GenerativeModel('gemini-1.5-flash')

response = model.generate_content(img)

to_markdown(response.text)

"""####To provide both text and images in a prompt, pass a list containing the strings and images:"""

response = model.generate_content(["Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.", img], stream=True)
response.resolve()

to_markdown(response.text)

"""##Chat conversations

Gemini enables you to have freeform conversations across multiple turns. The `ChatSession` class simplifies the process by managing the state of the conversation, so unlike with `generate_content`, I do not have to store the conversation history as a list.
Initialize the chat:
"""

model = genai.GenerativeModel('gemini-1.5-flash')
chat = model.start_chat(history=[])
chat

"""The `ChatSession.send_message` method returns the same `GenerateContentResponse` type as `GenerativeModel.generate_content`. It also appends my message and the response to the chat history:"""

response = chat.send_message("In one long sentence, explain how a computer works to a young child.")
to_markdown(response.text)

chat.history

"""One can keep sending messages to continue the conversation. Use the `stream=True` argument to stream the chat:"""

response = chat.send_message("Okay, how about a more detailed explanation to a high schooler?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

"""`genai.protos.Content` objects contain a list of `genai.protos.Part` objects that each contain either a text (string) or inline_data (`genai.protos.Blob`), where a blob contains binary data and a `mime_type`. The chat history is available as a list of `genai.protos.Content` objects in `ChatSession.history`:"""

for message in chat.history:
  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))

"""##Count tokens

Large language models have a context window, and the context length is often measured in terms of the number of tokens. With the Gemini API, one can determine the number of tokens per any `genai.protos.Content` object. In the simplest case, you can pass a query string to the `GenerativeModel.count_tokens` method as follows:
"""

model.count_tokens("What is the meaning of life?")

model.count_tokens(chat.history)

"""##Use Embeddings

####Embedding is a technique used to represent information as a list of floating point numbers in an array.
* With Gemini, one can represent text (words, sentences, and blocks of text) in a vectorized form, making it easier to compare and contrast embeddings.
* For example, two texts that share a similar subject matter or sentiment should have similar embeddings, which can be identified through mathematical comparison techniques such as cosine similarity.

Use the `embed_content` method to generate embeddings. The method handles embedding for the following tasks (`task_type`):

Task Type | Description
---       | ---
RETRIEVAL_QUERY	| Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a `title`.
SEMANTIC_SIMILARITY	| Specifies the given text will be used for Semantic Textual Similarity (STS).
CLASSIFICATION	| Specifies that the embeddings will be used for classification.
CLUSTERING	| Specifies that the embeddings will be used for clustering.

The following generates an embedding for a single string for document retrieval:
"""

result = genai.embed_content(
    model="models/text-embedding-004",
    content="What is the meaning of life?",
    task_type="retrieval_document",
    title="Embedding of single string")

# 1 input > 1 vector output
print(str(result['embedding'])[:50], '... TRIMMED]')

"""Note: The `retrieval_document` task type is the only task that accepts a title.
To handle batches of strings, pass a list of strings in content:
"""

result = genai.embed_content(
    model="models/text-embedding-004",
    content=[
      'What is the meaning of life?',
      'How much wood would a woodchuck chuck?',
      'How does the brain work?'],
    task_type="retrieval_document",
    title="Embedding of list of strings")

# A list of inputs > A list of vectors output
for v in result['embedding']:
  print(str(v)[:50], '... TRIMMED ...')

"""While the `genai.embed_content` function accepts simple strings or lists of strings, it is actually built around the `genai.protos.Content` type (like `GenerativeModel.generate_content`). `genai.protos.Content` objects are the primary units of conversation in the API.
While the `genai.protos.Content` object is multimodal, the `embed_content method` only supports text embeddings. This design gives the API the possibility to expand to multimodal embeddings.
"""

response.candidates[0].content

result = genai.embed_content(
    model = 'models/text-embedding-004',
    content = response.candidates[0].content)

# 1 input > 1 vector output
print(str(result['embedding'])[:50], '... TRIMMED ...')

"""Similarly, the chat history contains a list of `genai.protos.Content` objects, which you can pass directly to the `embed_content` function:"""

chat.history

result = genai.embed_content(
    model = 'models/text-embedding-004',
    content = chat.history)

# 1 input > 1 vector output
for i,v in enumerate(result['embedding']):
  print(str(v)[:50], '... TRIMMED...')

"""##Advanced Use Cases

The following sections discuss advanced use cases and lower-level details of the Python SDK for the Gemini API.

####Safety settings

1. The `safety_settings` argument lets you configure what the model blocks and allows in both prompts and responses.
2. By default, safety settings block content with medium and/or high probability of being unsafe content across all dimensions.
3. Enter a questionable prompt and run the model with the default safety settings, and it will not return any candidates:
"""

response = model.generate_content('[Questionable prompt here]')
response.candidates

"""The `prompt_feedback` will tell you which safety filter blocked the prompt:"""

response.prompt_feedback

"""Now provide the same prompt to the model with newly configured safety settings, and one may get a response."""

response = model.generate_content('[Questionable prompt here]',
                                  safety_settings={'HARASSMENT':'block_none'})
response.text

"""Also note that each candidate has its own `safety_ratings`, in case the prompt passes but the individual responses fail the safety checks.

##Encode messages

The previous sections relied on the SDK to make it easy for you to send prompts to the API. This section offers a fully-typed equivalent to the previous example, so you can better understand the lower-level details regarding how the SDK encodes messages.

The `google.generativeai.protos` submodule provides access to the low level classes used by the API behind the scenes:
The SDK attempts to convert your message to a `genai.protos.Content` object, which contains a list of `genai.protos.Part` objects that each contain either:
1. a text (`string`)
2. `inline_data` (`genai.protos.Blob`), where a blob contains binary data and
3. a `mime_type`.

You can also pass any of these classes as an equivalent dictionary.
##### Note: The only accepted mime types are some image types, `image/*`.
So, the `fully-typed` equivalent to the previous example is:
"""

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    genai.protos.Content(
        parts = [
            genai.protos.Part(text="Write a short, engaging blog post based on this picture."),
            genai.protos.Part(
                inline_data=genai.protos.Blob(
                    mime_type='image/jpeg',
                    data=pathlib.Path('image.jpg').read_bytes()
                )
            ),
        ],
    ),
    stream=True)

response.resolve()

to_markdown(response.text[:100] + "... [TRIMMED] ...")

"""##Multi-turn conversations

* While the `genai.ChatSession` class shown earlier can handle many use cases, it does make some assumptions.
* If one's use case doesn't fit into this chat implementation it's good to remember that `genai.ChatSession` is just a wrapper around `GenerativeModel.generate_content`.
* In addition to single requests, it can handle `multi-turn` conversations.
* The individual messages are genai.protos.Content objects or compatible dictionaries, as seen in previous sections.
* As a dictionary, the message requires role and parts keys. The role in a conversation can either be the user, which provides the prompts, or model, which provides the responses.
* Pass a list of genai.protos.Content objects and it will be treated as `multi-turn` chat:
"""

model = genai.GenerativeModel('gemini-1.5-flash')

messages = [
    {'role':'user',
     'parts': ["Briefly explain how a computer works to a young child."]}
]
response = model.generate_content(messages)

to_markdown(response.text)

"""To continue the conversation, add the response and another message.

Note: For multi-turn conversations, you need to send the whole conversation history with each request. The API is stateless.
"""

messages.append({'role':'model',
                 'parts':[response.text]})

messages.append({'role':'user',
                 'parts':["Okay, how about a more detailed explanation to a high school student?"]})

response = model.generate_content(messages)

to_markdown(response.text)

"""##Generation configuration

The `generation_config` argument allows you to modify the generation parameters. Every prompt you send to the model includes parameter values that control how the model generates responses.
"""

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'Tell me a story about a magic backpack.',
    generation_config=genai.types.GenerationConfig(
        # Only one candidate for now.
        candidate_count=1,
        stop_sequences=['x'],
        max_output_tokens=20,
        temperature=1.0)
)

text = response.text

if response.candidates[0].finish_reason.name == "MAX_TOKENS":
    text += '...'

to_markdown(text)